{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from postgres import connection\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(connection: psycopg2.extensions.connection, start_date = \"2009-12-01\", end_date = \"2022-12-30\") -> pd.DataFrame:\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT ticker FROM equities\")\n",
    "    tickers = cursor.fetchall()\n",
    "    stock_data = {}\n",
    "    null_volumes = {}\n",
    "\n",
    "    for stock_tuple in tickers:\n",
    "        stock = stock_tuple[0]\n",
    "        if stock == \"^STI\":\n",
    "            continue\n",
    "        table_name = f\"stock_{stock[:3]}\"  # Remove the .SI suffix\n",
    "        query = (\n",
    "            f\"SELECT * FROM {table_name} WHERE Date >= %s AND Date <= %s ORDER BY Date ASC\"\n",
    "        )\n",
    "        cursor.execute(query, (start_date, end_date))\n",
    "        data = cursor.fetchall()\n",
    "        if data:\n",
    "            dates, open_price, high, low, close, adj_close, volume = zip(*data)\n",
    "            \n",
    "            if datetime.strptime(start_date, \"%Y-%m-%d\") != datetime.strptime(dates[0], \"%Y-%m-%d\") \\\n",
    "                or datetime.strptime(end_date, \"%Y-%m-%d\") != datetime.strptime(dates[-1], \"%Y-%m-%d\"):\n",
    "                continue\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                'Date': pd.to_datetime(dates),\n",
    "                'Open': open_price,\n",
    "                'High': high,\n",
    "                'Low': low,\n",
    "                'Close': close,\n",
    "                'Adj_Close': adj_close,\n",
    "                'Volume': volume\n",
    "            })\n",
    "            if df[-1:]['Adj_Close'].values[0] < 0.2:\n",
    "                continue\n",
    "            # Handle special case where all columns are equal\n",
    "            mask = (df['Open'] == df['High']) & (df['High'] == df['Low']) & (df['Low'] == df['Close']) & (df['Close'] == df['Adj_Close'])\n",
    "            df.loc[mask, 'Volume'] = -1.0\n",
    "            \n",
    "            df.replace(0, np.nan, inplace=True)\n",
    "            \n",
    "            null_counts = df.isnull().sum()\n",
    "            if null_counts.any():\n",
    "                null_volumes[stock] = {'null_count': null_counts['Volume'], '0_vol': df[df['Volume'] == -1.0].shape[0]}  \n",
    "            stock_data[stock] = df\n",
    "\n",
    "    cursor.close()\n",
    "    return stock_data, null_volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data,null_volumes = download_data(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_volumes_df = pd.DataFrame.from_dict(null_volumes, orient='index', columns=[ 'null_count', '0_vol'])\n",
    "null_volumes_df.sort_values(by=['null_count'], ascending=False, inplace=True)\n",
    "#null_volumes_df.to_excel('null_volumes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {}\n",
    "training_data = {}\n",
    "for stock, df in all_data.items():\n",
    "    if stock in null_volumes:\n",
    "        test_data[stock] = df\n",
    "    else:\n",
    "        training_data[stock] = df\n",
    "        \n",
    "print(f\"Number of stocks in training data: {len(training_data)}\")\n",
    "print(f\"Number of stocks in test data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in training_data.items():\n",
    "    print(key)\n",
    "    print(value[value['Volume'] == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def wrangle_data(sequence, data_split, examples, batch_size):\n",
    "    \n",
    "    # Add extra data point for labels\n",
    "    examples += 1\n",
    "    \n",
    "    # Add a rank for the data points\n",
    "    # Current: Rank 1 with 100 data points -> Rank 2 with 100 examples of 1 data point\n",
    "    print('Sequence shape: ', np.shape(sequence))\n",
    "    seq_expand = tf.expand_dims(sequence, -1)\n",
    "    print('Sequence shape: ', np.shape(seq_expand))\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(seq_expand)\n",
    "    # Window and shift the data\n",
    "    dataset = dataset.window(examples, shift=1, drop_remainder=True)\n",
    "    # Convert from dataset of datasets to dataset of tensors\n",
    "    dataset = dataset.flat_map(lambda x: x.batch(examples))\n",
    "    \n",
    "    dataset = dataset.map(lambda x: (x[:-1], x[-1]))\n",
    "    \n",
    "    if data_split == 'train':\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    else:\n",
    "        dataset = dataset.cache()\n",
    "        \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(new_model, loss='mean_squared_error'):\n",
    "    #adam = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "    new_model.compile(optimizer='adam', loss=loss, metrics=['mean_absolute_error'])\n",
    "    print(new_model.summary())\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_rnn_model_multi(features):\n",
    "    new_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer((None, len(features))),\n",
    "        tf.keras.layers.Conv1D(30, kernel_size=6, padding='causal', activation='relu'),\n",
    "        tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(60),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    return compile_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, metrics=None):\n",
    "\n",
    "    if isinstance(metrics, str):\n",
    "        metrics = [metrics]\n",
    "    if metrics is None:\n",
    "        metrics = [x for x in history.history.keys() if x[:4] != 'val_']\n",
    "    if len(metrics) == 0:\n",
    "        print('No metrics to plot')\n",
    "        return\n",
    "    \n",
    "    x = history.epoch\n",
    "    \n",
    "    rows = 1\n",
    "    cols = len(metrics)\n",
    "    count = 0\n",
    "    \n",
    "    plt.figure(figsize=(12* cols, 8))\n",
    "    \n",
    "    for metric in sorted(metrics):\n",
    "        count+=1\n",
    "        plt.subplot(rows, cols, count)\n",
    "        plt.plot(x, history.history[metric], label='Train')\n",
    "        val_metric = f'val_{metric}'\n",
    "        if val_metric in history.history.keys():\n",
    "            plt.plot(x, history.history[val_metric], label='Validation')\n",
    "        plt.title(metric.capitalize())\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sequence(time, sequences, start=0, end=None):\n",
    "    y_max = 1.0\n",
    "    \n",
    "    if len(np.shape(sequences)) == 1:\n",
    "        print('Initial Shape', np.shape(sequences))\n",
    "        sequences = [sequences]\n",
    "        print('Shape: ', np.shape(sequences))\n",
    "    \n",
    "    time = time[start:end]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        print('Sequence shape: ', np.shape(sequence))\n",
    "        y_max = max(y_max, np.max(sequence))\n",
    "        sequence = sequence[start:end]\n",
    "        plt.plot(time, sequence)\n",
    "        \n",
    "    plt.ylim(-2, y_max)\n",
    "    plt.xlim(time[start], time[-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(trained_model, predict_sequence, true_values, predict_time, begin=0, end=None):\n",
    "    predictions = trained_model.predict(predict_sequence)\n",
    "    print(predictions.shape)\n",
    "    predictions = predictions[:,-1].reshape(len(predictions))\n",
    "    print(predictions.shape)\n",
    "    plot_sequence(predict_time, (true_values, predictions), begin, end)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, time, split_size):\n",
    "    d_split = int(np.ceil(len(data) * split_size))\n",
    "    \n",
    "    big_data = data[:d_split]\n",
    "    big_time = time[:d_split]\n",
    "    small_data = data[d_split:]\n",
    "    small_time = time[d_split:]\n",
    "    \n",
    "    return big_data, big_time, small_data, small_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Open', 'High', 'Low', 'Close',  'Adj_Close']\n",
    "model = stacked_rnn_model_multi(features)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "all_data_concat = pd.concat(all_data.values())\n",
    "data_to_predict = np.array(all_data_concat[features])\n",
    "length = len(data_to_predict)\n",
    "time_steps = np.array(range(length))\n",
    "month = 21\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rescaler = StandardScaler()\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_to_predict_normalizer = scaler.fit_transform(data_to_predict)\n",
    "adj_close = rescaler.fit_transform(data_to_predict[:,4].reshape(-1, 1))\n",
    "train_adj_close, train_time_steps, test_adj_close, test_time_steps = split_data(data_to_predict_normalizer, time_steps, 0.8)\n",
    "train_adj_close, train_time_steps, val_adj_close, val_time_steps = split_data(train_adj_close, train_time_steps, 0.9)\n",
    "\n",
    "train_data = wrangle_data(train_adj_close, 'train', month, 32)\n",
    "val_data = wrangle_data(val_adj_close, 'val', month, 32)\n",
    "test_data = wrangle_data(test_adj_close, 'test', month, 32)\n",
    "\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    history = model.fit(train_data, epochs=100, validation_data=val_data, callbacks=[early_stop], use_multiprocessing=True, workers=12)\n",
    "plot_history(history)\n",
    "model.evaluate(test_data)\n",
    "\n",
    "# Adjust prediction and plotting to handle multiple features\n",
    "predictions = model.predict(test_data)\n",
    "predictions = predictions[:,-1].reshape(-1, 1)\n",
    "predictions = rescaler.inverse_transform(predictions)\n",
    "test_volume_true = test_adj_close[month:,][:,0].reshape(-1, 1)\n",
    "test_volume_true = rescaler.inverse_transform(test_volume_true) \n",
    "predictions = predictions.reshape(-1)\n",
    "test_volume_true = test_volume_true.reshape(-1)\n",
    "show_predictions(model, test_data, test_volume_true, test_time_steps[month:], end=2*month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
